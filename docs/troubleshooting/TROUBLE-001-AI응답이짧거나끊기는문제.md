# TROUBLE-001-AI응답이짧거나끊기는문제

## 문제 상황
현재 Gemini API에서 gemini-flash-2.5 모델을 사용해 AI 응답을 생성하고 있다.  
이 과정에서 질문이 모호한 경우, 응답이 문장 중간에서 끊기는 문제가 발생했다.

예를 들어  
"개발자에 대해서 어떻게 생각해?" 와 같은 질문에서는

> "개발자 준비를 어떻게 해야 할지 막막할 수 있습니다. 하지만 체계적으로 접근하면 충분"

과 같이 문장이 완결되지 않은 상태로 응답이 종료되었다.

반면  
"스프링 부트가 실행이 안되는데 로그를 보내면 될까?" 처럼  
상황과 목적이 비교적 명확한 질문에서는 정상적인 응답이 생성되었다.

## 가설
1. 프롬프트가 구체적이지 않아 모델이 최소한의 답변만 생성했을 가능성
2. 프론트엔드 렌더링 과정에서 응답이 잘렸을 가능성
3. 백엔드에서 여러 텍스트 중 첫 번째 텍스트만 출력하고 있을 가능성
4. API 호출 시 설정한 maxOutputTokens 값의 문제일 가능성

## 시도
1. **프롬프트 문제 검증**
    - 번호 개수, 출력 형식, 설명 길이를 명시한 프롬프트로 수정해 테스트
2. **프론트엔드 렌더링 문제 검증**
    - Postman으로 API를 직접 호출해 응답 확인
    - 브라우저 개발자 도구를 통해 실제 전달되는 응답 데이터 확인
3. **백엔드 처리 문제 검증**
    - Gemini 응답 구조인 `candidates[].content.parts[]`를 로그로 출력해 전체 응답 확인
4. **maxOutputTokens 설정 검증**
    - maxOutputTokens 값을 512, 1024, 2048로 변경하며 응답 결과 비교

## 결과
1. **프롬프트 문제**
    - 프롬프트를 구체화해도 응답이 중간에서 끊기는 현상은 개선되지 않음
2. **프론트엔드 렌더링 문제**
    - Postman과 개발자 도구 모두에서 이미 끊긴 응답이 내려오는 것을 확인
    - 프론트엔드 렌더링 문제가 아님을 확인
3. **백엔드 처리 문제**
    - 응답 메시지는 `index = 0`에 정상적으로 존재
    - 백엔드에서 텍스트를 누락하거나 잘라내는 로직은 없음을 확인
4. **maxOutputTokens 문제**
    - 512, 1024에서는 여전히 응답이 문장 중간에서 종료됨
    - 2048로 설정했을 때에만 응답이 정상적으로 끝까지 생성됨

## 결론
이 문제는 프롬프트나 프론트엔드 문제가 아니라  
**LLM 출력 토큰 상한이 응답을 완성하기에 충분하지 않았던 문제**로 판단했다.

- 512, 1024 토큰은 모호한 질문에 대해 모델이 응답을 완결하기에 부족했다.
- 2048 토큰 설정 시에만 응답이 안정적으로 완성되었다.
- 따라서 서버 측에서 maxOutputTokens를 고정값으로 두기보다,
  질문의 성격이나 응답 길이에 따라 **유동적으로 조절하는 방식**이 필요하다고 결론지었다.

이 경험을 통해  
LLM 연동에서는 “토큰을 최소로 유지하는 것”보다  
“응답이 완결될 수 있는 최소 토큰을 확보하는 것”이  
안정성 측면에서 더 중요하다는 점을 확인했다.
